# Import the ChatOllama class from the langchain_ollama module
# This class allows you to interact with Ollama models like Llama 3 in Python
from langchain_ollama import ChatOllama

# Import message types from langchain_core.messages
# - HumanMessage: represents a message sent by the user
# - SystemMessage: represents instructions or context for the AI system
from langchain_core.messages import HumanMessage, SystemMessage


# Initialize the Ollama language model
# - 'model' specifies which model to use; here it's "llama3.2"
# - 'temperature' controls randomness in responses (0 = deterministic, higher = more creative)
llm = ChatOllama(
    model="llama3.2",
    temperature=1
)

# Create a list called 'messages' to store the conversation
# - SystemMessage sets the context or role for the AI (here: "Act like a Pirate")
# - HumanMessage simulates the user's input to the AI
messages = [
    SystemMessage("Act like a Pirate"),
    HumanMessage("Hi, how are you today?"),
]

# Invoke the model with the 'messages' list
# - The model will process the conversation and generate a response
# - 'result' will store the AI's response object
result = llm.invoke(messages)

# Print the AI-generated response to the console
# - 'result.content' contains the actual text generated by the model
print(result.content)
